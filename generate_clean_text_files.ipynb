{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import pymupdf\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import notebook\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_output_to_file(papers: List[Dict[str, str]], stats: Optional[Dict[str, int]], output_file: str) -> None:\n",
    "    \"\"\"Save papers and statistics to a JSON file.\"\"\"\n",
    "    output_data = {\n",
    "        \"papers\": papers,\n",
    "        \"statistics\": stats\n",
    "    }\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "\n",
    "def _generate_and_save_stats(\n",
    "        papers: List[Dict[str, str]],\n",
    "        generate_stats: bool,\n",
    "        save_output: bool,\n",
    "        output_file: str\n",
    ") -> Optional[Dict[str, int]]:\n",
    "    \"\"\"Generate statistics from papers and save to file if requested.\"\"\"\n",
    "    if not generate_stats:\n",
    "        return None\n",
    "\n",
    "    paper_count = len(papers)\n",
    "\n",
    "    # Prepare statistics\n",
    "    stats = {\n",
    "        \"total_papers\": paper_count,\n",
    "        \"average_word_count\": 0,  \n",
    "        \"min_word_count\": 0,  \n",
    "        \"max_word_count\": 0, \n",
    "    }\n",
    "\n",
    "    if paper_count > 0:\n",
    "        # Calculate statistics using word count if available\n",
    "        word_counts = [len(paper['text'].split()) for paper in papers]  # Calculate word count from text\n",
    "\n",
    "        total_word_count = sum(word_counts)\n",
    "        stats[\"average_word_count\"] = total_word_count / paper_count\n",
    "        stats[\"min_word_count\"] = min(word_counts)\n",
    "        stats[\"max_word_count\"] = max(word_counts)\n",
    "\n",
    "    # Save statistics and papers to output file\n",
    "    if save_output:\n",
    "        _save_output_to_file(papers, stats, output_file)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def _extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file, handling potential exceptions.\"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "        return text\n",
    "    except pymupdf.pymupdf.FileDataError:\n",
    "        print(f\"Error: Unable to open or read the PDF file: {pdf_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def _debug_print(step, before, after, on=False):\n",
    "    if on:\n",
    "        print(f\"Debug: {step}\")\n",
    "        print(f\"Before Text Count: {len(before)} characters\")\n",
    "        print(f\"After Text Count: {len(after)} characters\")\n",
    "        print(f\"Before:\\n{before}\\n\")\n",
    "        print(f\"After:\\n{after}\\n\")\n",
    "\n",
    "def process_papers(data_folder_path: str, generate_stats: bool = True, save_output: bool = True) -> Tuple[List[Dict[str, str]], Optional[Dict[str, int]]]:\n",
    "    \"\"\"Process all PDF papers in the given folder and its subfolders, optionally generate statistics.\"\"\"\n",
    "    papers = []\n",
    "\n",
    "    # Traverse the directory tree\n",
    "    for root, _, files in os.walk(data_folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".pdf\"):  # Ensure case-insensitive matching\n",
    "                file_path = os.path.join(root, filename)  # Use root for the correct path\n",
    "                text = _extract_text_from_pdf(file_path)\n",
    "\n",
    "                if text:  # Only add papers that were successfully extracted\n",
    "                    papers.append({\"id\": filename, \"text\": text})\n",
    "\n",
    "    # Save papers to a fixed pickle file path\n",
    "    with open(\"papers.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(papers, pickle_file)\n",
    "\n",
    "    # Optionally generate and save statistics\n",
    "    stats = None\n",
    "    if generate_stats:\n",
    "        stats = _generate_and_save_stats(papers, generate_stats, save_output, \"process_stats.json\")\n",
    "\n",
    "    return papers, stats\n",
    "\n",
    "def clean_papers(\n",
    "        papers: List[Dict[str, str]],\n",
    "        debug_mode,\n",
    "        model_name: str = \"distilbert-base-uncased-finetuned-conll03-english\",  # Default model name\n",
    "        generate_stats: bool = True,\n",
    "        save_output: bool = True\n",
    ") -> Tuple[List[Dict[str, str]], Optional[Dict[str, int]]]:\n",
    "    \"\"\"Clean the extracted text using a specified Hugging Face model for Named Entity Recognition (NER) and regex\n",
    "    expressions.\"\"\"\n",
    "    if debug_mode:\n",
    "        print(\"!!!Debug mode enabled!!!\")\n",
    "\n",
    "    cleaned_papers = []\n",
    "\n",
    "    # Construct the model path assuming models are stored in a folder called 'models'\n",
    "    model_path = os.path.join(\"models\", model_name)\n",
    "\n",
    "    # Check if the model is present in the models directory\n",
    "    if not os.path.exists(model_path):\n",
    "        # Download the model and tokenizer\n",
    "        print(f\"Model '{model_name}' not found in 'models' directory. Downloading...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    else:\n",
    "        # Load the tokenizer and model from the local path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "    # Create the NER pipeline\n",
    "    nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    count = 0\n",
    "    for paper in papers:\n",
    "        if debug_mode and count == 10:\n",
    "            print(\"Stop cleaning because debug mode is enabled\")\n",
    "            break\n",
    "        # Initialize\n",
    "        text = paper['text']\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Dictionary of compiled regex patterns and their replacements\n",
    "        patterns = {\n",
    "            'non_ascii': re.compile(r'[^\\x00-\\x7F]+'),\n",
    "            'isbn': re.compile(r'\\b(?:isbn(?:-1[03])?:? )?(?=[-0-9xX ]{13,17})(97[89][- ]?)?[0-9]{1,5}[- ]?[0-9]+[- ]?[0-9]+[- ]?[0-9xX]\\b'),\n",
    "            'url': re.compile(r'http\\S+|www\\S+|https\\S+'),\n",
    "            'email': re.compile(r'\\S+@\\S+'),\n",
    "            'reference': re.compile(r'\\[\\d+\\]'),\n",
    "            'allowed_chars': re.compile(r'[^A-Za-z0-9.,?!:;\"(){}\\[\\]<>@#$%^&*_+=/\\\\|~\\s]'),\n",
    "            'anchors': re.compile(r'\\b(?:vol\\.|no\\.|fig\\.|pp\\.|p\\.|pg\\.|table)\\s\\d+\\b')\n",
    "        }\n",
    "\n",
    "        # Loop through the patterns and apply the substitutions\n",
    "        for key, pattern in patterns.items():\n",
    "            text_before = text\n",
    "            text = pattern.sub('', text)\n",
    "            _debug_print(text_before, text, key)\n",
    "\n",
    "        # Remove ellipses (three or more dots)\n",
    "        text = re.sub(r'\\.\\.\\.+', '', text)\n",
    "\n",
    "        # Remove empty lines, lines with just numbers, lines with less than 5 characters, and lines without any letters\n",
    "        text = \"\\n\".join([line for line in text.split(\"\\n\") if line.strip() != \"\" and not line.strip().isdigit() and len(line.strip()) >= 5 and re.search(r'[a-zA-Z]', line)])\n",
    "\n",
    "        # Use the NER pipeline to process the cleaned text\n",
    "        entities = nlp_pipeline(text)\n",
    "\n",
    "        # Create a list of entities to be removed, e.g., locations and organizations\n",
    "        types = ['B-LOC']\n",
    "        # types = ['B-LOC', 'B-ORG']\n",
    "        entities_to_remove = {ent['word'] for ent in entities if ent['entity'] in types and len(ent['word']) >= 3 and not ent['word'].startswith('##')}\n",
    "        print(f'Entities to remove: {entities_to_remove}')\n",
    "\n",
    "        # Remove identified entities\n",
    "        for entity in entities_to_remove:\n",
    "            text = text.replace(entity, '')\n",
    "\n",
    "        cleaned_papers.append({\"id\": paper['id'], \"text\": text})\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Optionally generate and save statistics\n",
    "    stats = None\n",
    "    if generate_stats:\n",
    "        stats = _generate_and_save_stats(cleaned_papers, generate_stats, save_output, \"clean_stats.json\")\n",
    "\n",
    "    return cleaned_papers, stats\n",
    "\n",
    "def save_texts_to_files(texts: List[Dict[str, str]], output_path: str) -> int:\n",
    "    num_files_saved = 0\n",
    "    \n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # # Check if there is at least one file in the folder\n",
    "    # if any(os.scandir(output_path)):\n",
    "    #     # Prompt the user once for confirmation to overwrite all files\n",
    "    #     overwrite = input(f\"The folder '{output_path}' already contains files. Overwrite all? (y/n): \").strip().lower()\n",
    "    #     if overwrite != 'y':\n",
    "    #         print(\"Skipping save operation\")\n",
    "    #         return num_files_saved\n",
    "\n",
    "    # Loop through every dictionary in the list\n",
    "    for element in texts:\n",
    "        # Create a safe filename by replacing any unsupported characters\n",
    "        title = element[\"id\"]\n",
    "        text = element[\"text\"]\n",
    "        \n",
    "        # Replace unsupported characters with underscores\n",
    "        safe_filename = re.sub(r'[^a-zA-Z0-9]', '_', title)\n",
    "        if not safe_filename:  # Ensure the filename is not empty\n",
    "            print(f\"Skipping empty filename for title: {title}\")\n",
    "            continue\n",
    "            \n",
    "        filename = f\"{safe_filename}.txt\"\n",
    "        file_path = os.path.join(output_path, filename)\n",
    "\n",
    "        try:\n",
    "            # Write the text to the file\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(text)\n",
    "            num_files_saved += 1\n",
    "            print(f\"Saved '{title}' to '{file_path}'\")  # Uncomment to see confirmation of saved files\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save '{title}': {e}\")\n",
    "    \n",
    "    return num_files_saved\n",
    "\n",
    "@contextmanager\n",
    "def time_step():\n",
    "    # Startup code\n",
    "    # print(\"Setting up timer\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Teardown code\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"***Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "data_folder_path = \"data\"\n",
    "output_path = \"cleaned_text\"\n",
    "file_stats = {}\n",
    "loaded_papers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "# Check if the pickle file exists\n",
    "if os.path.exists(\"papers.pkl\") and DEBUG_MODE:\n",
    "    with open(\"papers.pkl\", \"rb\") as pickle_file:\n",
    "        papers = pickle.load(pickle_file)\n",
    "    print(\"Loaded processed papers\")\n",
    "    loaded_papers = True\n",
    "else:\n",
    "    print(\"Processing papers...\")\n",
    "    with time_step():\n",
    "        papers, process_stats = process_papers(data_folder_path)\n",
    "        file_stats[\"num_processed_papers\"] = len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean (!!!check DEBUG_MODE flag for clean_papers function!!!)\n",
    "print(\"Cleaning papers...\")\n",
    "with time_step():\n",
    "    cleaned_papers: List[Dict[str, str]]\n",
    "    clean_stats: Dict[str, int] \n",
    "    cleaned_papers, clean_stats = clean_papers(papers, debug_mode=DEBUG_MODE)\n",
    "    file_stats[\"num_cleaned_papers\"] = len(cleaned_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output statistics\n",
    "if not loaded_papers:\n",
    "    if process_stats:\n",
    "        with time_step():\n",
    "            print(\"Process statistics:\", process_stats)\n",
    "\n",
    "    if clean_stats:\n",
    "        with time_step():\n",
    "            print(\"Clean statistics:\", clean_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save texts\n",
    "print(\"Saving cleaned papers...\")\n",
    "with time_step():\n",
    "    num_saved_papers = save_texts_to_files(cleaned_papers, output_path)\n",
    "    file_stats[\"num_saved_papers\"] = num_saved_papers\n",
    "\n",
    "print(file_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
